@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first ICML},
  pages={1},
  year={2004},
  organization={ACM}
}
@article{abbeel2010autonomous,
  title={Autonomous helicopter aerobatics through apprenticeship learning},
  author={Abbeel, Pieter and Coates, Adam and Ng, Andrew Y},
  journal={IJRR},
  year={2010},
  publisher={SAGE Publications}
}
@article{barto2003recent,
  title={Recent advances in hierarchical reinforcement learning},
  author={Barto, Andrew G and Mahadevan, Sridhar},
  journal={Discrete Event Dynamic Systems},
  volume={13},
  number={4},
  pages={341--379},
  year={2003},
  publisher={Springer}
}
@article{ballard2013hierarchical,
  title={A hierarchical modular architecture for embodied cognition},
  author={Ballard, Dana H and Kit, Dmitry and Rothkopf, Constantin A and Sullivan, Brian},
  journal={Multisensory research},
  volume={26},
  pages={177--204},
  year={2013}
}

@inproceedings{konidaris2009skill,
  title={Skill discovery in continuous reinforcement learning domains using skill chaining},
  author={Konidaris, George and Barreto, Andre S},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1015--1023},
  year={2009}
}

@inproceedings{bhat2006difficulty,
  title={On the difficulty of modular reinforcement learning for real-world partial programming},
  author={Bhat, Sooraj and Isbell, Charles L and Mateas, Michael},
  booktitle={Proceedings of the National Conference on Artificial Intelligence},
  volume={21},
  number={1},
  pages={318},
  year={2006},
  organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}
@article{boyan1995generalization,
  title={Generalization in reinforcement learning: Safely approximating the value function},
  author={Boyan, Justin and Moore, Andrew W},
  journal={In NIPS},
  pages={369--376},
  year={1995},
  publisher={Morgan Kaufmann Publishers}
}
@inproceedings{choi2011map,
  title={MAP inference for Bayesian inverse reinforcement learning},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  booktitle={NIPS},
  pages={1989--1997},
  year={2011}
}
@inproceedings{choi2012nonparametric,
  title={Nonparametric bayesian inverse reinforcement learning for multiple reward functions},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  booktitle={NIPS},
  pages={305--313},
  year={2012}
}
@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={J. Artif. Intell. Res.(JAIR)},
  volume={13},
  pages={227--303},
  year={2000}
}
@incollection{dimitrakakis2012bayesian,
  title={Bayesian multitask inverse reinforcement learning},
  author={Dimitrakakis, Christos and Rothkopf, Constantin A},
  booktitle={Recent Advances in Reinforcement Learning},
  pages={273--284},
  year={2012},
  publisher={Springer}
}
@article{doya2002multiple,
  title={Multiple model-based reinforcement learning},
  author={Doya, Kenji and Samejima, Kazuyuki and Katagiri, Ken-ichi and Kawato, Mitsuo},
  journal={Neural computation},
  volume={14},
  number={6},
  pages={1347--1369},
  year={2002},
  publisher={MIT Press}
}

@article{gershman2009human,
  title={Human reinforcement learning subdivides structured action spaces by learning effector-specific values},
  author={Gershman, Samuel J and Pesaran, Bijan and Daw, Nathaniel D},
  journal={The Journal of Neuroscience},
  volume={29},
  number={43},
  pages={13524--13531},
  year={2009},
  publisher={Soc Neuroscience}
}
@inproceedings{guestrin2001multiagent,
  title={Multiagent Planning with Factored MDPs.},
  author={Guestrin, Carlos and Koller, Daphne and Parr, Ronald},
  booktitle={NIPS},
  volume={1},
  pages={1523--1530},
  year={2001}
}
@article{guestrin2003efficient,
  title={Efficient solution algorithms for factored MDPs},
  author={Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
  journal={Journal of Artificial Intelligence Research},
  pages={399--468},
  year={2003}
}

@article{haruno2004neural,
  title={A neural correlate of reward-based behavioral learning in caudate nucleus: a functional magnetic resonance imaging study of a stochastic decision task},
  author={Haruno, Masahiko and Kuroda, Tomoe and Doya, Kenji and Toyama, Keisuke and Kimura, Minoru and Samejima, Kazuyuki and Imamizu, Hiroshi and Kawato, Mitsuo},
  journal={The Journal of Neuroscience},
  volume={24},
  number={7},
  pages={1660--1665},
  year={2004},
  publisher={Soc Neuroscience}
}
@article{holroyd2002neural,
  title={The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity.},
  author={Holroyd, Clay B and Coles, Michael GH},
  journal={Psychological review},
  volume={109},
  number={4},
  pages={679},
  year={2002},
  publisher={American Psychological Association}
}
@article{humphrys1996action,
  title={Action selection methods using reinforcement learning},
  author={Humphrys, Mark},
  journal={From Animals to Animats},
  volume={4},
  pages={135--144},
  year={1996}
}
@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={arXiv preprint cs/9605103},
  year={1996}
}
@article{kawato2007efficient,
  title={Efficient reinforcement learning: computational theories, neuroscience and robotics},
  author={Kawato, Mitsuo and Samejima, Kazuyuki},
  journal={Current opinion in neurobiology},
  volume={17},
  number={2},
  pages={205--212},
  year={2007},
  publisher={Elsevier}
}
@inproceedings{levine2011nonlinear,
  title={Nonlinear inverse reinforcement learning with Gaussian processes},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  booktitle={NIPS},
  pages={19--27},
  year={2011}
}
@article{muelling2014learning,
  title={Learning strategies in table tennis using inverse reinforcement learning},
  author={Muelling, Katharina and Boularias, Abdeslam and Mohler, Betty and Sch{\"o}lkopf, Bernhard and Peters, Jan},
  journal={Biological cybernetics},
  volume={108},
  number={5},
  pages={603--619},
  year={2014},
  publisher={Springer}
}
@article{neu2012apprenticeship,
  title={Apprenticeship learning using inverse reinforcement learning and gradient methods},
  author={Neu, Gergely and Szepesv{\'a}ri, Csaba},
  journal={UAI},
  year={2012}
}
@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J and others},
  booktitle={ICML},
  pages={663--670},
  year={2000}
}
@inproceedings{ono1996multi,
  title={Multi-agent reinforcement learning: A modular approach},
  author={Ono, Norihiko and Fukumoto, Kenji},
  booktitle={Proceedings of the Second International Conference on Multi-Agent Systems},
  pages={252--258},
  year={1996}
}
@inproceedings{ramachandran2007bayesian,
  title={Bayesian inverse reinforcement learning},
  author={Ramachandran, Deepak and Amir, Eyal},
  booktitle={Proceedings of the 20th IJCAI},
  pages={2586--2591},
  year={2007},
  organization={Morgan Kaufmann Publishers Inc.}
}
@inproceedings{ring2011q,
  title={Q-error as a selection mechanism in modular reinforcement-learning systems},
  author={Ring, Mark and Schaul, Tom},
  booktitle={IJCAI},
  volume={22},
  number={1},
  pages={1452},
  year={2011}
}
@inproceedings{rohanimanesh2005coarticulation,
  title={Coarticulation: an approach for generating concurrent plans in Markov decision processes},
  author={Rohanimanesh, Khashayar and Mahadevan, Sridhar},
  booktitle={Proceedings of the 22nd ICML},
  pages={720--727},
  year={2005},
  organization={ACM}
}
@article{rothkopf2013modular,
  title={Modular inverse reinforcement learning for visuomotor behavior},
  author={Rothkopf, Constantin A and Ballard, Dana H},
  journal={Biological cybernetics},
  volume={107},
  number={4},
  pages={477--490},
  year={2013},
  publisher={Springer}
}

@inproceedings{russell2003q,
  title={Q-decomposition for reinforcement learning agents},
  author={Russell, Stuart and Zimdars, Andrew},
  booktitle={ICML},
  pages={656--663},
  year={2003}
}

@article{samejima2003inter,
  title={Inter-module credit assignment in modular reinforcement learning},
  author={Samejima, Kazuyuki and Doya, Kenji and Kawato, Mitsuo},
  journal={Neural Networks},
  volume={16},
  number={7},
  pages={985--994},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{sprague2003multiple,
  title={Multiple-goal reinforcement learning with modular sarsa (0)},
  author={Sprague, Nathan and Ballard, Dana},
  booktitle={IJCAI},
  pages={1445--1447},
  year={2003},
  organization={Citeseer}
}
@article{sprague2007modeling,
  title={Modeling embodied visual behaviors},
  author={Sprague, Nathan and Ballard, Dana and Robinson, Al},
  journal={ACM Transactions on Applied Perception (TAP)},
  volume={4},
  number={2},
  pages={11},
  year={2007},
  publisher={ACM}
}
@article{storn1997differential,
  title={Differential evolution--a simple and efficient heuristic for global optimization over continuous spaces},
  author={Storn, Rainer and Price, Kenneth},
  journal={Journal of global optimization},
  volume={11},
  number={4},
  pages={341--359},
  year={1997},
  publisher={Springer}
}
@book{sutton1998introduction,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT Press}
}
@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{thomas2012motor,
  title={Motor primitive discovery.},
  author={Thomas, Philip S and Barto, Andrew G},
  booktitle={ICDL-EPIROB},
  pages={1--8},
  year={2012}
}
@inproceedings{uchibe1996behavior,
  title={Behavior coordination for a mobile robot using modular reinforcement learning},
  author={Uchibe, Eiji and Asada, Minoru and Hosoda, Koh},
  booktitle={IROS},
  volume={3},
  pages={1329--1336},
  year={1996},
  organization={IEEE}
}
@inproceedings{vogel2012improving,
  title={Improving Hybrid Vehicle Fuel Efficiency Using Inverse Reinforcement Learning.},
  author={Vogel, Adam and Ramachandran, Deepak and Gupta, Rakesh and Raux, Antoine},
  booktitle={AAAI},
  year={2012}
}
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{ziebart2008maximum,
  title={Maximum Entropy Inverse Reinforcement Learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={AAAI},
  pages={1433--1438},
  year={2008}
}
@inproceedings{ziebart2009planning,
  title={Planning-based prediction for pedestrians},
  author={Ziebart, Brian D and Ratliff, Nathan and Gallagher, Garratt and Mertz, Christoph and Peterson, Kevin and Bagnell, James A and Hebert, Martial and Dey, Anind K and Srinivasa, Siddhartha},
  booktitle={IROS},
  pages={3931--3936},
  year={2009},
  organization={IEEE}
}


@article{Tong2014,
author = {Tong, M. H. and Hayhoe, M. M.},
doi = {10.1167/14.10.5},
issn = {1534-7362},
journal = {Journal of Vision},
month = aug,
number = {10},
pages = {5},
title = {{Modeling uncertainty and intrinsic reward in a virtual walking task}},
url = {http://www.journalofvision.org/content/14/10/5.abstract?sid=91add15e-5e77-4b92-a372-db0eb76af183},
volume = {14},
year = {2014}
}

@ARTICLE{Rothkopf12Infer,
AUTHOR={Rothkopf, Constantin A},   
TITLE={Inferring human intrinsic rewards through inverse reinforcement learning},      
JOURNAL={Frontiers in Computational Neuroscience},      
VOLUME={},      
YEAR={2013},
NUMBER={50},     
URL={http://www.frontiersin.org/computational_neuroscience/10.3389/conf.fncom.2012.55.00050/full},       
DOI={10.3389/conf.fncom.2012.55.00050},      
ISSN={1662-5188} ,      
}

@book{storn1995differential,
  title={Differential evolution-a simple and efficient adaptive scheme for global optimization over continuous spaces},
  author={Storn, Rainer and Price, Kenneth},
  volume={3},
  year={1995},
  publisher={ICSI Berkeley}
}

@incollection{stone2000layered,
  title={Layered learning},
  author={Stone, Peter and Veloso, Manuela},
  booktitle={Machine Learning: ECML 2000},
  pages={369--381},
  year={2000},
  publisher={Springer}
}

@incollection{macalpine2015ut,
  title={UT Austin Villa: RoboCup 2014 3D simulation league competition and technical challenge champions},
  author={MacAlpine, Patrick and Depinet, Mike and Liang, Jason and Stone, Peter},
  booktitle={RoboCup 2014: Robot World Cup XVIII},
  pages={33--46},
  year={2015},
  publisher={Springer}
}

@article{hansen2003reducing,
  title={Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)},
  author={Hansen, Nikolaus and M{\"u}ller, Sibylle D and Koumoutsakos, Petros},
  journal={Evolutionary Computation},
  volume={11},
  number={1},
  pages={1--18},
  year={2003},
  publisher={MIT Press}
}

@incollection{ng2006autonomous,
  title={Autonomous inverted helicopter flight via reinforcement learning},
  author={Ng, Andrew Y and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
  booktitle={Experimental Robotics IX},
  pages={363--372},
  year={2006},
  publisher={Springer}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@inproceedings{jonsson2005causal,
  title={A causal approach to hierarchical decomposition of factored MDPs},
  author={Jonsson, Anders and Barto, Andrew},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={401--408},
  year={2005},
  organization={ACM}
}

