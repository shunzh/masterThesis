Reinforcement learning has shown advantages in learning with only the task
specifications and feedback signals. A learning agent can interact with the
task, observe a current state, and take an action to observe
transition to the next state and receive a reward signal. The underlying model is either
known or not by the agent. Concretely, the task is formulated as a
Markov Decision Process (MDP), which will be defined in details later.
The objective of the task is to take actions to maximize the accumulated payoff.
In recent years, reinforcement learning has been successfully applied to
helicopter control \cite{ng2006autonomous} and Atari games \cite{mnih2013playing}.

Reinforcement learning is analogous to the way that human learns. Human infants
have limited prior knowledge of the world. They can learn to optimize
their behavior by observing the environment and perceiving joy or pain. So it is
of interest to discover the underlying decision model of human. In both fields
of artificial intelligence and neuroscience, it is believed that reinforcement
learning is the model that can explain humans' learning.

However, humans are able to learn and accomplish complex tasks more efficiently than
reinforcement learning agents. The possibilities are that humans have some prior
knowledge on how to accomplish smaller and easier tasks. When they tackle a
complex task, they only need to decide which basic skills they need to use, and
how to combine them. Taking driving for example, humans learning to drive don't
take it as a completely new task. Many of their preliminary skills can
contribute to this new task. It is a complex behavior but a combination of
object avoidance, object following, etc.

To understand how human subjects make decisions and to test our hypothesis, we
propose an novel inverse reinforcement learning approach in this thesis.
We first tested the correctness and efficiency of this approach in a toy domain,
and further collected human data in a navigation task. We used such method
to analyze how the behavior is generated.

This thesis is organized as follows. Chapter~\ref{chp:mirl} introduces the
preliminary concepts of reinforcement learning, inverse reinforcement learning,
modular reinforcement learning and describes the proposed algorithm.
Chapter~\ref{chp:lr} reviews the literature on task decomposition and inverse
reinforcement learning. We report our experimental results in
Chapter~\ref{chp:eval} and conclude in Chapter~\ref{chp:conclude}.
